{"id": "C2-1437", "contents": "Deep reinforcement learning is poised to revolutionise the field of AI and represents a step towards building\nautonomous systems with a higher level understanding of the\nvisual world. Currently, deep learning is enabling reinforcement\nlearning to scale to problems that were previously intractable,\nsuch as learning to play video games directly from pixels. Deep\nreinforcement learning algorithms are also applied to robotics,\nallowing control policies for robots to be learned directly from\ncamera inputs in the real world. In this survey, we begin with\nan introduction to the general field of reinforcement learning,\nthen progress to the main streams of value-based and policybased methods. Our survey will cover central algorithms in\ndeep reinforcement learning, including the deep Q-network,\ntrust region policy optimisation, and asynchronous advantage\nactor-critic. In parallel, we highlight the unique advantages of\ndeep neural networks, focusing on visual understanding via\nreinforcement learning. To conclude, we describe several current\nareas of research within the field.\nI. INTRODUCTION\nOne of the primary goals of the field of artificial intelligence\n(AI) is to produce fully autonomous agents that interact with\ntheir environments to learn optimal behaviours, improving over\ntime through trial and error. Crafting AI systems that are\nresponsive and can effectively learn has been a long-standing\nchallenge, ranging from robots, which can sense and react\nto the world around them, to purely software-based agents,\nwhich can interact with natural language and multimedia.\nA principled mathematical framework for experience-driven\nautonomous learning is reinforcement learning (RL) [135]. Although RL had some successes in the past [141, 129, 62, 93],\nprevious approaches lacked scalablity and were inherently\nlimited to fairly low-dimensional problems. These limitations\nexist because RL algorithms share the same complexity issues as other algorithms: memory complexity, computational\ncomplexity, and in the case of machine learning algorithms,\nsample complexity [133]. What we have witnessed in recent\nyears\u2014the rise of deep learning, relying on the powerful\nfunction approximation and representation learning properties\nof deep neural networks\u2014has provided us with new tools to\novercoming these problems.\nThe advent of deep learning has had a significant impact\non many areas in machine learning, dramatically improving\nthe state-of-the-art in tasks such as object detection, speech\nrecognition, and language translation [70]. The most important\nproperty of deep learning is that deep neural networks can\nautomatically find compact low-dimensional representations\n(features) of high-dimensional data (e.g., images, text and\naudio). Through crafting inductive biases into neural network\narchitectures, particularly that of hierarchical representations,\nmachine learning practitioners have made effective progress\nin addressing the curse of dimensionality [15]. Deep learning\nhas similarly accelerated progress in RL, with the use of\ndeep learning algorithms within RL defining the field of\n\u201cdeep reinforcement learning\u201d (DRL). The aim of this survey\nis to cover both seminal and recent developments in DRL,\nconveying the innovative ways in which neural networks can\nbe used to bring us closer towards developing autonomous\nagents. For a more comprehensive survey of recent efforts in\nDRL, including applications of DRL to areas such as natural\nlanguage processing [106, 5], we refer readers to the overview\nby Li [78].\nDeep learning enables RL to scale to decision-making\nproblems that were previously intractable, i.e., settings with\nhigh-dimensional state and action spaces. Amongst recent\nwork in the field of DRL, there have been two outstanding\nsuccess stories. The first, kickstarting the revolution in DRL,\nwas the development of an algorithm that could learn to play\na range of Atari 2600 video games at a superhuman level,\ndirectly from image pixels [84]. Providing solutions for the\ninstability of function approximation techniques in RL, this\nwork was the first to convincingly demonstrate that RL agents\ncould be trained on raw, high-dimensional observations, solely\nbased on a reward signal. The second standout success was\nthe development of a hybrid DRL system, AlphaGo, that\ndefeated a human world champion in Go [128], paralleling the\nhistoric achievement of IBM\u2019s Deep Blue in chess two decades\nearlier [19] and IBM\u2019s Watson DeepQA system that beat the\nbest human Jeopardy! players [31]. Unlike the handcrafted\nrules that have dominated chess-playing systems, AlphaGo\nwas composed of neural networks that were trained using\nsupervised and reinforcement learning, in combination with\na traditional heuristic search algorithm.\nDRL algorithms have already been applied to a wide range\nof problems, such as robotics, where control policies for robots\ncan now be learned directly from camera inputs in the real\nworld [74, 75], succeeding controllers that used to be handengineered or learned from low-dimensional features of the\nrobot\u2019s state. In a step towards even more capable agents,\nDRL has been used to create agents that can meta-learn (\u201clearn\nto learn\u201d) [29, 156], allowing them to generalise to complex\nvisual environments they have never seen before [29]. In\nFigure 1, we showcase just some of the domains that DRL\nhas been applied to, ranging from playing video games [84]\nto indoor navigation [167].\nVideo games may be an interesting challenge, but learning\nhow to play them is not the end goal of DRL. One of the\ndriving forces behind DRL is the vision of creating systems\nthat are capable of learning how to adapt in the real world.\nFrom managing power consumption [142] to picking and\nstowing objects [75], DRL stands to increase the amount\nof physical tasks that can be automated by learning. However, DRL does not stop there, as RL is a general way of\napproaching optimisation problems by trial and error. From\ndesigning state-of-the-art machine translation models [168] to\nconstructing new optimisation functions [76], DRL has already\nbeen used to approach all manner of machine learning tasks"}