{"id": "C2-1436", "contents": "Deep learning has revolutionized many machine\nlearning tasks in recent years, ranging from image classification\nand video processing to speech recognition and natural language\nunderstanding. The data in these tasks are typically represented\nin the Euclidean space. However, there is an increasing number\nof applications where data are generated from non-Euclidean domains and are represented as graphs with complex relationships\nand interdependency between objects. The complexity of graph\ndata has imposed significant challenges on existing machine\nlearning algorithms. Recently, many studies on extending deep\nlearning approaches for graph data have emerged. In this survey,\nwe provide a comprehensive overview of graph neural networks\n(GNNs) in data mining and machine learning fields. We propose\na new taxonomy to divide the state-of-the-art graph neural\nnetworks into four categories, namely recurrent graph neural\nnetworks, convolutional graph neural networks, graph autoencoders, and spatial-temporal graph neural networks. We further\ndiscuss the applications of graph neural networks across various\ndomains and summarize the open source codes, benchmark data\nsets, and model evaluation of graph neural networks. Finally,\nwe propose potential research directions in this rapidly growing\nfield.\nIndex Terms\u2014Deep Learning, graph neural networks, graph\nconvolutional networks, graph representation learning, graph\nautoencoder, network embedding\nI. INTRODUCTION\nT\nHE recent success of neural networks has boosted research on pattern recognition and data mining. Many\nmachine learning tasks such as object detection [1], [2],\nmachine translation [3], [4], and speech recognition [5], which\nonce heavily relied on handcrafted feature engineering to\nextract informative feature sets, has recently been revolutionized by various end-to-end deep learning paradigms, e.g.,\nconvolutional neural networks (CNNs) [6], recurrent neural\nnetworks (RNNs) [7], and autoencoders [8]. The success of\ndeep learning in many domains is partially attributed to the\nrapidly developing computational resources (e.g., GPU), the\navailability of big training data, and the effectiveness of deep\nlearning to extract latent representations from Euclidean data\n(e.g., images, text, and videos). Taking image data as an\nZ. Wu, F. Chen, G. Long, C. Zhang are with Centre for Artificial Intelligence, FEIT, University of Technology Sydney, NSW 2007, Australia (Email: zonghan.wu-3@student.uts.edu.au; fengwen.chen@student.uts.edu.au;\nguodong.long@uts.edu.au; chengqi.zhang@uts.edu.au).\nS. Pan is with Faculty of Information Technology, Monash University,\nClayton, VIC 3800, Australia (Email: shirui.pan@monash.edu).\nP. S. Yu is with Department of Computer Science, University of Illinois at\nChicago, Chicago, IL 60607-7053, USA (Email: psyu@uic.edu)\nCorresponding author: Shirui Pan.\nManuscript received Dec xx, 2018; revised Dec xx, 201x.\nexample, we can represent an image as a regular grid in\nthe Euclidean space. A convolutional neural network (CNN)\nis able to exploit the shift-invariance, local connectivity, and\ncompositionality of image data [9]. As a result, CNNs can\nextract local meaningful features that are shared with the entire\ndata sets for various image analysis.\nWhile deep learning effectively captures hidden patterns of\nEuclidean data, there is an increasing number of applications\nwhere data are represented in the form of graphs. For examples, in e-commence, a graph-based learning system can\nexploit the interactions between users and products to make\nhighly accurate recommendations. In chemistry, molecules\nare modeled as graphs, and their bioactivity needs to be\nidentified for drug discovery. In a citation network, papers\nare linked to each other via citationships and they need to\nbe categorized into different groups. The complexity of graph\ndata has imposed significant challenges on existing machine\nlearning algorithms. As graphs can be irregular, a graph may\nhave a variable size of unordered nodes, and nodes from a\ngraph may have a different number of neighbors, resulting\nin some important operations (e.g., convolutions) being easy\nto compute in the image domain, but difficult to apply to\nthe graph domain. Furthermore, a core assumption of existing\nmachine learning algorithms is that instances are independent\nof each other. This assumption no longer holds for graph data\nbecause each instance (node) is related to others by links of\nvarious types, such as citations, friendships, and interactions.\nRecently, there is increasing interest in extending deep\nlearning approaches for graph data. Motivated by CNNs,\nRNNs, and autoencoders from deep learning, new generalizations and definitions of important operations have been\nrapidly developed over the past few years to handle the complexity of graph data. For example, a graph convolution can\nbe generalized from a 2D convolution. As illustrated in Figure\n1, an image can be considered as a special case of graphs\nwhere pixels are connected by adjacent pixels. Similar to 2D\nconvolution, one may perform graph convolutions by taking\nthe weighted average of a node\u2019s neighborhood information.\nThere are a limited number of existing reviews on the topic\nof graph neural networks (GNNs). Using the term geometric\ndeep learning, Bronstein et al. [9] give an overview of deep\nlearning methods in the non-Euclidean domain, including\ngraphs and manifolds. Although it is the first review on GNNs,\nthis survey mainly reviews convolutional GNNs. Hamilton\net al. [10] cover a limited number of GNNs with a focus\non addressing the problem of network embedding. Battaglia\net al. [11] position graph networks as the building blocks"}